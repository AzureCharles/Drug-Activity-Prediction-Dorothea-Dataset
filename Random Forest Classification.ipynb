{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d8a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当有本地csv数据时，使用以下方法获取数据\n",
    "def csvLoader(csvpath):\n",
    "    csv_data = pd.read_csv(csvpath,header=0)\n",
    "    #print(csv_data)\n",
    "    # 转换为numpy矩阵\n",
    "    return np.array(csv_data)\n",
    "\n",
    "PCA_projected_trainData = csvLoader('PCAProjectedTrainData800.csv')\n",
    "PCA_projected_testData = csvLoader('PCAProjectedTestData800.csv')\n",
    "#print(PCA_projected_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43279124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '1' '0' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0' '1'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '1' '1' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '1' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "train_labels=[]\n",
    "#with open(trainingFile, \"r\") as fr1:\n",
    "#    trainFile = fr1.readlines()\n",
    "def getListFromFile(_filename):\n",
    "    _list = []\n",
    "    with open(_filename, \"r\") as fr3:\n",
    "        dat_r = fr3.readlines()\n",
    "    for inputData in dat_r:\n",
    "        _list.append(inputData[0])\n",
    "    target = np.asarray(_list)\n",
    "    return target\n",
    "\n",
    "test_labels = getListFromFile('valid_labels.dat')\n",
    "for c in getListFromFile('dorothea_train.labels'):\n",
    "    if c!= '1':\n",
    "        train_labels.append('0')\n",
    "    else:train_labels.append(c)\n",
    "train_labels=np.asarray(train_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b742b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data handled by PCA\n",
    "def pcaData2csv(csvpath1,csvpath2, train_data,test_data,_sep=','):\n",
    "    dftrain=pd.DataFrame(train_data)\n",
    "    dftest=pd.DataFrame(test_data)\n",
    "    dftrain.to_csv(csvpath1,index=False,sep=_sep)\n",
    "    dftest.to_csv(csvpath2,index=False,sep=_sep)\n",
    "    if os.path.exists(csvpath1):\n",
    "        print(\"PCA projected train data has been written into csv files in:\"+str(csvpath1))\n",
    "        if os.path.exists(csvpath2):\n",
    "            print(\"PCA projected test data has been written into csv files in:\"+str(csvpath2))\n",
    "        else:\n",
    "            raise Warning(\"File written failed in:\"+str(csvpath2))\n",
    "    else:raise Warning(\"File written failed in:\"+str(csvpath1))\n",
    "    # 抛出dataframe类型以备其他用途\n",
    "    return dftrain,dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0744e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA projected train data has been written into csv files in:trainLables.csv\n",
      "PCA projected test data has been written into csv files in:testLabels.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     0\n",
       " 0    0\n",
       " 1    0\n",
       " 2    0\n",
       " 3    0\n",
       " 4    0\n",
       " ..  ..\n",
       " 795  0\n",
       " 796  0\n",
       " 797  0\n",
       " 798  0\n",
       " 799  1\n",
       " \n",
       " [800 rows x 1 columns],\n",
       "      0\n",
       " 0    0\n",
       " 1    0\n",
       " 2    1\n",
       " 3    0\n",
       " 4    0\n",
       " ..  ..\n",
       " 345  0\n",
       " 346  1\n",
       " 347  0\n",
       " 348  0\n",
       " 349  0\n",
       " \n",
       " [350 rows x 1 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels2=train_labels.astype(int)\n",
    "test_labels2=test_labels.astype(int)\n",
    "path1=r\"trainLables.csv\"\n",
    "path2=r\"testLabels.csv\"\n",
    "#print(len(train_labels2),train_labels2)\n",
    "# Also gotten by dorothea_train.labels→train_labels2(binary)\n",
    "#print(len(test_labels2),test_labels2)\n",
    "# Also gotten by dorothea_valid.labels→test_labels2(binary)\n",
    "pcaData2csv(path1,path2, train_labels2,test_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c877930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c25820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, recall_score\n",
    "#Read the input files and read every line\n",
    "def loadData(trainingFile, testingFile):\n",
    "    \n",
    "    def convertDataframe(inputFile):\n",
    "        data = pd.DataFrame(columns=range(100000))\n",
    "        \n",
    "        for i in range(len(inputFile)):\n",
    "            record = np.fromstring(inputFile[i], dtype=int, sep=' ')\n",
    "            record_bool = [0 for j in range(100000)]\n",
    "            for col in record:\n",
    "                record_bool[col-1] = 1\n",
    "            \n",
    "            data.loc[i] = record_bool\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    with open(trainingFile, \"r\") as fr1:\n",
    "        trainFile = fr1.readlines()\n",
    "    \n",
    "    #Split each line in the two files into label and data  \n",
    "    train_data_list = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    for inputData in trainFile:\n",
    "        train_labels_list.append(inputData[0])\n",
    "        #Remove the activity label (0/1) and new line character from each record\n",
    "        inputData = inputData.replace(\"0\\t\", \"\")\n",
    "        inputData = inputData.replace(\"1\\t\", \"\")\n",
    "        inputData = inputData.replace(\"\\n\", \"\")\n",
    "        train_data_list.append(inputData)\n",
    "    \n",
    "    train_labels = np.asarray(train_labels_list)\n",
    "    train_data = convertDataframe(train_data_list)\n",
    "        \n",
    "    with open(testingFile, \"r\") as fr2:\n",
    "        testFile = fr2.readlines()\n",
    "    \n",
    "    test_data = convertDataframe(testFile)\n",
    "            \n",
    "    return train_data, test_data, train_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Project data on a reduced dimensionality k using PCA\n",
    "def pca(train_data, test_data, k):\n",
    "\n",
    "    pca = sklearnPCA(n_components = k)\n",
    "    PCA_projected_trainData = pca.fit_transform(train_data)\n",
    "    PCA_projected_testData = pca.transform(test_data)\n",
    "    \n",
    "    return PCA_projected_trainData, PCA_projected_testData\n",
    "\n",
    "def lda(train_data,train_labels, test_data,test_labels, k):\n",
    "    # 创建线性判别分析对象，并指定降维后的维度为2\n",
    "    lda = LinearDiscriminantAnalysis(n_components = k)\n",
    "\n",
    "    # 对数据进行降维，并查看方差比例\n",
    "    LDA_projected_trainData = lda.fit_transform(train_data,train_labels)\n",
    "    LDA_projected_testData = lda.fit_transform(test_data,test_labels)\n",
    "    print(lda.transform(test_data))\n",
    "    print(lda.explained_variance_ratio_)\n",
    "    return LDA_projected_trainData, LDA_projected_testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the training and the test data set and get 3 separate dataframes of training reviews, test reviews and training labels\n",
    "train_data, test_data, train_labels = loadData('train.dat', 'test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the number of dimensions from 100000 to 800 using PCA\n",
    "train_labels = train_labels.astype(int)\n",
    "test_labels = np.array(loadFile('valid_labels.dat')).astype(int)\n",
    "PCA_projected_trainData, PCA_projected_testData = pca(train_data, test_data, 800)\n",
    "LDA_projected_trainData, LDA_projected_testData = lda(train_data,train_labels, test_data,test_labels, 1)\n",
    "print(PCA_projected_trainData, PCA_projected_testData)\n",
    "print(LDA_projected_trainData, LDA_projected_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path1=r\"PCAProjectedTrainData800.csv\"\n",
    "csv_path2=r\"PCAProjectedTestData800.csv\"\n",
    "pcaData2csv(csv_path1,csv_path2, PCA_projected_trainData,PCA_projected_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Data = np.vstack((PCA_projected_trainData,PCA_projected_testData))\n",
    "all_labels = np.hstack((train_labels,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "threshold = np.max(LDA_projected_testData)-0.5*(np.max(LDA_projected_testData)-np.min(LDA_projected_testData))\n",
    "pos_count = np.count_nonzero(LDA_projected_testData > threshold)\n",
    "neg_count = np.count_nonzero(LDA_projected_testData < threshold)\n",
    "print(pos_count, neg_count)\n",
    "\n",
    "# 根据阈值设置颜色\n",
    "colors = np.ravel(np.where(LDA_projected_testData > threshold, 'red', 'blue'))\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(LDA_projected_testData, np.zeros(np.size(LDA_projected_testData,0)), c=colors)\n",
    "plt.axvline(threshold, color='k', linestyle='--') # 绘制阈值线\n",
    "plt.xlabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# 导入sklearn.metrics模块\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "# 定义真实标签和预测标签\n",
    "y_pred = []\n",
    "for i in range(np.size(LDA_projected_testData,0)):\n",
    "    if LDA_projected_testData[i,0]>threshold:\n",
    "        y_pred.append(1)\n",
    "    else:y_pred.append(0)\n",
    "# 计算准确率\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "print(\"Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "# 计算精确率\n",
    "precision = precision_score(test_labels, y_pred)\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "\n",
    "# 计算召回率\n",
    "recall = recall_score(test_labels, y_pred)\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "\n",
    "# 计算F1分数\n",
    "f1 = f1_score(test_labels, y_pred)\n",
    "print(\"F1-score: {:.4f}\".format(f1))\n",
    "\n",
    "# 计算ROC曲线和AUC\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"ROC curve: fpr = {}, tpr = {}, AUC = {:.4f}\".format(fpr, tpr, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, average_precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def resultAnalysis(testData,testLabels,pred,proba,_module=None):\n",
    "    xmodulelist = ['macro','micro','weighted','non-avg']\n",
    "    criteion = ['Accuracy','Precision','Recall','F1_score','AUC','AUPRC']\n",
    "    df = pd.DataFrame(index=xmodulelist,columns=criteion)\n",
    "    \n",
    "    \n",
    "    if _module not in xmodulelist:\n",
    "        \n",
    "        df['Accuracy']['non-avg']=round(accuracy_score(testLabels,pred),3)\n",
    "        df['Precision']['non-avg']=round(precision_score(testLabels,pred,),3)\n",
    "        df['Recall']['non-avg']=round(recall_score(testLabels,pred,),3)\n",
    "        df['AUC']['non-avg']=round(roc_auc_score(testLabels,proba,),3)\n",
    "        df['AUPRC']['non-avg']=round(average_precision_score(testLabels,pred,),3)\n",
    "        df['F1_score']['non-avg']=round(f1_score(testLabels,pred,),3)\n",
    "        print(df.iloc[3,:])\n",
    "            \n",
    "    else:\n",
    "        df['Accuracy'][_module]=round(accuracy_score(testLabels,pred),3)\n",
    "        df['Precision'][_module]=round(precision_score(testLabels,pred,average=_module),3)\n",
    "        df['Recall'][_module]=round(recall_score(testLabels,pred,average=_module),3)\n",
    "        df['AUC'][_module]=round(roc_auc_score(testLabels,proba,average=_module),3)\n",
    "        df['AUPRC'][_module]=round(average_precision_score(testLabels,pred,average=_module),3)\n",
    "        df['F1_score'][_module]=round(f1_score(testLabels,pred,average=_module),3)\n",
    "        print(df.loc[_module,:])\n",
    "    print(confusion_matrix(testLabels,pred))\n",
    "    return classification_report(testLabels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38640f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def BNBC2(trainData,trainLabels, testData,testLabels):\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    clf = BernoulliNB (alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "    clf.fit(trainData,trainLabels)\n",
    "    \n",
    "    pred = clf.predict(testData)\n",
    "    proba = clf.predict_proba(testData)[:,1]\n",
    "    score = clf.score(testData,testLabels)\n",
    "    fpr, tpr, thresholds = roc_curve(testLabels, proba)\n",
    "    rocauc=auc(fpr, tpr)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    #print(proba)\n",
    "    print(\"\\tAccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\tPrecision:{:.3f}\".format(precision_score(testLabels,pred)))\n",
    "    print(\"\\tRecall:{:.3f}\".format(recall_score(testLabels,pred)))\n",
    "    print(\"\\tF1_score:{:.3f}\".format(f1_score(testLabels,pred)))\n",
    "    print(\"\\tAUC:{:.3f}\".format(roc_auc_score(testLabels,proba)))\n",
    "    return pred\n",
    "\n",
    "BNBC2(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e48a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "def min_max_normalization(np_array):\n",
    "    min_max_scaler = MinMaxScaler(feature_range=[0,10])\n",
    "    ret = min_max_scaler.fit_transform(np_array)\n",
    "    return ret\n",
    "\n",
    "def CNB(trainData,trainLabels, testData,testLabels):\n",
    "\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    trainData=min_max_normalization(trainData)\n",
    "    testData=min_max_normalization(testData)\n",
    "    \n",
    "    clf = ComplementNB(alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
    "    clf.fit(trainData,trainLabels)\n",
    "\n",
    "    pred = clf.predict(testData)\n",
    "    proba = clf.predict_proba(testData)[:,1]\n",
    "    score = clf.score(testData,testLabels)\n",
    "    fpr, tpr, thresholds = roc_curve(testLabels, proba)\n",
    "    #rocauc=auc(fpr, tpr)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    # 以下针对少数类\n",
    "    print(\"\\tAccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\tPrecision:{:.3f}\".format(precision_score(testLabels,pred)))\n",
    "    print(\"\\tRecall:{:.3f}\".format(recall_score(testLabels,pred)))\n",
    "    print(\"\\tAUC:{:.3f}\".format(roc_auc_score(testLabels,proba)))\n",
    "    return pred\n",
    "\n",
    "CNB(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn import svm\n",
    "#from sklearn.metrics import roc_curve, roc_auc_score, recall_score, precision_score\n",
    "#from imblearn import specificity_score\n",
    "\n",
    "def SVMC_kernel(trainData,trainLabels, testData,testLabels, weights='balanced'):\n",
    "    #clf = svm.SVC(gamma='scale', class_weight=weights)\n",
    "    #clf.fit(trainData,trainLabels)\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    for k in ['linear','rbf','poly','sigmoid']:\n",
    "        clf_proba = svm.SVC(kernel=k,C=100,probability=True).fit(trainData,trainLabels)\n",
    "        #print(clf_proba.decision_function(testData).shape)\n",
    "        #predictions = clf.predict(testData)\n",
    "        clf_pred = clf_proba.predict(testData)\n",
    "        result = clf_proba.score(testData, testLabels)\n",
    "        score = clf_proba.decision_function(testData)\n",
    "\n",
    "        rocauc = roc_auc_score(testLabels, score)\n",
    "        recall = recall_score(testLabels, clf_pred)\n",
    "        precision = precision_score(testLabels, clf_pred)\n",
    "        print(\"The kernel {} prediction is:\".format(k))\n",
    "        print(resultAnalysis(testData,testLabels,clf_pred,score,))\n",
    "    #print(\"\\tAccuracy:{:.3f}\".format(result))\n",
    "    #print(\"\\tPrecision:{:.3f}\".format(precision))\n",
    "    #print(\"\\tRecall:{:.3f}\".format(recall))\n",
    "    #print(\"\\tSpecificity:{:.3f}\".format(specificity_score(testLabels, clf_pred)))\n",
    "    #print(\"\\tAUC:{:.3f}\".format(rocauc))\n",
    "    #return clf_pred\n",
    "\n",
    "SVMC_kernel(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMC(trainData,trainLabels, testData,testLabels,weights='balanced'):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    clf_proba = svm.SVC(gamma='scale', class_weight=weights).fit(trainData,trainLabels)\n",
    "    #print(clf_proba.decision_function(testData).shape)\n",
    "    #predictions = clf.predict(testData)\n",
    "    \n",
    "    clf_pred = clf_proba.predict(testData)\n",
    "    \n",
    "    result = clf_proba.score(testData, testLabels)\n",
    "    score = clf_proba.decision_function(testData)\n",
    "    \n",
    "    rocauc = roc_auc_score(testLabels, score)\n",
    "    recall = recall_score(testLabels, clf_pred)\n",
    "    precision = precision_score(testLabels, clf_pred)\n",
    "    \n",
    "    print(resultAnalysis(testData,testLabels,clf_pred,score,))\n",
    "    #print(\"\\tAccuracy:{:.3f}\".format(result))\n",
    "    #print(\"\\tPrecision:{:.3f}\".format(precision))\n",
    "    #print(\"\\tRecall:{:.3f}\".format(recall))\n",
    "    #print(\"\\tSpecificity:{:.3f}\".format(specificity_score(testLabels, clf_pred)))\n",
    "    #print(\"\\tAUC:{:.3f}\".format(rocauc))\n",
    "    return clf_pred\n",
    "SVMC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier     #随机森林\n",
    "\n",
    "def RFC(trainData,trainLabels, testData,testLabels):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=63, max_depth=20,max_features=26,min_samples_leaf=3,min_samples_split=14,criterion='entropy',\n",
    "                                 class_weight='balanced',random_state=42)                      #不调参\n",
    "    rfc = rfc.fit(trainData, trainLabels)                 #用训练集数据训练模型\n",
    "    pred = rfc.predict(testData)\n",
    "    result = rfc.score(testData, testLabels)\n",
    "    proba = rfc.predict_proba(testData)[:,1]\n",
    "    rocauc = roc_auc_score(testLabels, proba)\n",
    "    recall = recall_score(testLabels, pred)\n",
    "    precision = precision_score(testLabels, pred)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    \n",
    "    return pred\n",
    "RFC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06670335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier     #随机森林\n",
    "def RFC2(trainData,trainLabels, testData,testLabels,paramdict):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    #paramdict={'n_estimators':63, 'max_depth':5,'criterion':'entropy','class_weight':'balanced','random_state':42}\n",
    "    rfc = RandomForestClassifier(n_estimators=paramdict['n_estimators'], \n",
    "                                 max_features=paramdict['max_features'],\n",
    "                                 min_samples_split=paramdict['min_samples_split'],\n",
    "                                 min_samples_leaf=paramdict['min_samples_leaf'], \n",
    "                                 min_weight_fraction_leaf=paramdict['min_weight_fraction_leaf'],\n",
    "                                 criterion=paramdict['criterion'],\n",
    "                                 class_weight='balanced',random_state=paramdict['random_state']) \n",
    "    #rfc = RandomForestClassifier(n_estimators=63, max_depth=5,criterion='entropy',class_weight='balanced',random_state=42)\n",
    "    start = time()\n",
    "    rfc = rfc.fit(trainData, trainLabels)                 #用训练集数据训练模型\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train model RF: %.9f seconds\" % elapsed)\n",
    "    pred = rfc.predict(testData)\n",
    "    result = rfc.score(testData, testLabels)\n",
    "    proba = rfc.predict_proba(testData)[:,1]\n",
    "    rocauc = roc_auc_score(testLabels, proba)\n",
    "    recall = recall_score(testLabels, pred)\n",
    "    precision = precision_score(testLabels, pred)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    \n",
    "    return pred\n",
    "#GS.fit(PCA_projected_trainData, train_labels)\n",
    "#print(GS.best_params_)\n",
    "#print(GS.best_score_)\n",
    "\n",
    "parameters={'n_estimators':63, 'max_depth':20,'max_features':26,'min_samples_split':14, 'min_samples_leaf':3,'min_weight_fraction_leaf':0.00,\n",
    "            'criterion':'entropy','class_weight':'balanced','random_state':42}\n",
    "RFC2(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a796806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from time import *\n",
    "\n",
    "#def load_csv(path):\n",
    "#    df = pd.read_csv(path)\n",
    "#    target = df['cls']\n",
    "#    df = df.drop(['cls'], axis=1)\n",
    "#    return xgb.DMatrix(df.values, label=target.values)\n",
    "\n",
    "#dtest = load_csv('/hdd/hdd1/twonormData/t1000.csv')\n",
    "#xgb.DMatrix(PCA_projected_trainData, label=train_labels.astype(int))\n",
    "def evaluate(trainData,trainLabels, testData,testLabels, num_trees=100,max_depth=50,num_jobs=-1):\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)\n",
    "    dtrain = xgb.DMatrix(trainData, label=trainLabels)\n",
    "    dtest = xgb.DMatrix(testData, label=testLabels)\n",
    "    param = {'num_parallel_tree':num_trees, 'max_depth':max_depth, 'objective':'binary:logistic',\n",
    "        'nthread':num_jobs, 'subsample':0.7,'scale_pos_weight':9}\n",
    "    start = time()\n",
    "    model = xgb.train(param, dtrain, 1)\n",
    "    # 性能评估以XGboost为例\n",
    "    #xgb = xgb.XGBClassifier()\n",
    "    # 对训练集训练模型\n",
    "    #xgb.fit(trainData,trainLabels)\n",
    "    # 对测试集进行预测\n",
    "    #y_pred = xgb.predict(testData)\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train XGBoost model: %.9f seconds\" % elapsed)\n",
    "    prediction = model.predict(dtest)\n",
    "    #proba = model.predict_proba(testData)[:,1]\n",
    "    #print(prediction.shape)\n",
    "    length=len(prediction)\n",
    "    proba = np.stack([[1-a for a in prediction],prediction],axis=-1)\n",
    "    pred=[]\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] >0.5:\n",
    "            pred.append(1)\n",
    "        else:pred.append(0)\n",
    "    \n",
    "    #print(pred)\n",
    "    #print(\"Accuracy = %.3f\" % np.mean(prediction == dtest.get_label()))\n",
    "    print(resultAnalysis(testData,testLabels,pred,prediction))\n",
    "\n",
    "evaluate(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be993fd5",
   "metadata": {},
   "source": [
    "调参是一个非常重要的步骤，可以帮助我们优化机器学习算法的表现。对于sklearn随机森林算法，调参需要注意以下几点：\n",
    "\n",
    "n_estimators：随机森林中树的数量。增加这个参数可以提高算法的表现，但同时会使得算法的计算时间变长。因此，需要根据具体情况进行调整。\n",
    "\n",
    "max_depth：树的最大深度。增加这个参数可以提高算法的表现，但同时也会使得算法的计算时间变长。需要注意的是，如果max_depth设置太大，会导致过拟合的问题。\n",
    "\n",
    "min_samples_split：节点进行分裂所需的最小样本数量。增加这个参数可以防止过拟合，但会使得算法的表现变差。\n",
    "\n",
    "min_samples_leaf：叶节点所需的最小样本数量。增加这个参数可以防止过拟合，但会使得算法的表现变差。\n",
    "\n",
    "max_features：特征随机选择的个数。增加这个参数可以提高算法的表现，但也会增加算法的计算时间。\n",
    "\n",
    "bootstrap：是否有放回地进行采样。如果设置为False，会使得随机森林变成极端随机森林。\n",
    "\n",
    "criterion：衡量分裂质量的准则。一般选择“gini”或“entropy”。\n",
    "\n",
    "## Grid Search-网格搜索\n",
    "调参的过程可以使用网格搜索(Grid Search)或随机搜索(Random Search)等方法进行。可以通过如下代码进行调参：\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# 设置要调整的参数\n",
    "parameters = {'n_estimators': [10, 50, 100],\n",
    "              'max_depth': [1, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2],\n",
    "              'max_features': [2, 4],\n",
    "              'bootstrap': [True, False],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# 使用网格搜索进行调参\n",
    "grid_search = GridSearchCV(clf, parameters, cv=3, n_jobs=1, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 输出最佳的参数组合和得分\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "```\n",
    "在网格搜索的过程中，我们首先创建了一个空的随机森林分类器，然后设置了要调整的参数。我们还设置了交叉验证的次数(cv)和并行处理的数量(n_jobs)。然后我们使用GridSearchCV函数进行网格搜索，并使用了verbose参数指定详细程度。最后输出最佳的参数组合和得分。\n",
    "\n",
    "## 贝叶斯优化\n",
    "scikit-learn里的内置贝叶斯优化库为BayesSearchCV，该库可以在超参数空间中使用朴素贝叶斯优化算法，以尝试找到最佳超参数组合。BayesSearchCV需要设定要优化的超参数的搜索范围和目标指标，并为算法提供模型评估器。\n",
    "\n",
    "下面是一个简单的随机森林模型的贝叶斯优化调参示例代码：\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# 定义随机森林模型并设定超参数空间\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "param_dist = {'max_depth': Integer(10, 50),\n",
    "              'min_samples_split': Integer(2, 10),\n",
    "              'min_samples_leaf': Integer(1, 10),\n",
    "              'max_features': Real(0.1, 1.0)}\n",
    "\n",
    "# 使用交叉验证来对模型进行评估\n",
    "def objective_func(params):\n",
    "    rf.set_params(**params)\n",
    "    return -np.mean(cross_val_score(rf, X, y, cv=5, scoring='f1'))\n",
    "\n",
    "# 在超参数空间上运用贝叶斯搜索来优化模型\n",
    "opt = BayesSearchCV(rf,\n",
    "                    param_dist,\n",
    "                    n_iter=25,\n",
    "                    scoring='f1',\n",
    "                    cv=5,\n",
    "                    random_state=0)\n",
    "opt.fit(X, y)\n",
    "```\n",
    "在上面的代码中，使用了sklearn的内置贝叶斯优化库BayesSearchCV。首先定义了一个随机森林模型并设定了超参数空间，然后使用交叉验证评估模型的表现，并将评估结果作为目标函数。最后，在超参数空间上使用BayesSearchCV进行参数搜索，搜索结束后可以使用如下代码来输出参数搜索的结果：\n",
    "\n",
    "```python\n",
    "print(\"Best F1 Score:3f\" opt.best_score_)\n",
    "print(\"Best Parameters:s\" str(opt.best_params_))\n",
    "```\n",
    "使用BayesSearchCV时，可以通过参数n_iter来控制贝叶斯优化的迭代次数，通常n_iter要比随机搜索大，这样可以更好地探索超参数空间。同时，参数cv指定了用于交叉验证的数据集的拆分数量，一般默认为5。scoring参数用于指定评估指标，这里选择了F1得分，因为它同时考虑了准确率和召回率，是一个比较全面的指标。\n",
    "\n",
    "需要注意的是，由于贝叶斯优化是一种基于概率的优化方法，因此它的寻优速度可能会受到几个因素的影响，例如超参数的数量和范围、目标函数的复杂性等。因此，在进行调参时，需要根据实际情况和计算资源情况进行适当的调整和控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from functools import partial\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "X=PCA_projected_trainData\n",
    "y=train_labels.astype(int)\n",
    "\n",
    "# 定义随机森林模型并设定超参数空间\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {'max_depth': Integer(1, 20),\n",
    "              'n_estimators': Integer(100, 250),\n",
    "              'min_samples_split': Integer(2, 10),\n",
    "              'min_samples_leaf': Integer(1, 10),\n",
    "              'min_weight_fraction_leaf': Real(0, 0.5),\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              }\n",
    "\n",
    "# 使用交叉验证来对模型进行评估\n",
    "def objective_func(params):\n",
    "    rf.set_params(**params)\n",
    "    return -np.mean(cross_val_score(rf, X, y, cv=10, scoring='f1'))\n",
    "\n",
    "# 在超参数空间上运用贝叶斯搜索来优化模型\n",
    "opt = BayesSearchCV(rf,\n",
    "                    param_dist,\n",
    "                    n_iter=35,\n",
    "                    scoring='recall',\n",
    "                    cv=10,\n",
    "                    random_state=42)\n",
    "opt.fit(X, y)\n",
    "print(\"Best F1 Score:%3f\" % opt.best_score_)\n",
    "print(\"Best Parameters:%s\" % str(opt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 调参，绘制学习曲线来调参n_estimators（对随机森林影响最大）\n",
    "score_lt = []\n",
    "\n",
    "# 每隔10步建立一个随机森林，获得不同n_estimators的得分\n",
    "for i in range(50,200,10):\n",
    "    rfc = RandomForestClassifier(n_estimators=i+1,random_state=42)\n",
    "    score = cross_val_score(rfc, PCA_Data, all_labels.astype(int), cv=10,scoring='roc_auc').mean()\n",
    "    score_lt.append(score)\n",
    "score_max = max(score_lt)\n",
    "print('最大得分：{}'.format(score_max),\n",
    "      '子树数量为：{}'.format(score_lt.index(score_max)*10+1))\n",
    "\n",
    "# 绘制学习曲线\n",
    "x = np.arange(51,201,10)\n",
    "plt.subplot(111)\n",
    "plt.plot(x, score_lt, 'r-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
