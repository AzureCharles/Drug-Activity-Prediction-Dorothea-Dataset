{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.12587939e+00 -1.28394947e-01 -1.02922805e-01 ... -1.07955584e+00\n",
      "   1.07551099e+00  4.22669750e-15]\n",
      " [-1.30842129e+00  1.93540142e-01 -4.05118163e-01 ... -1.56165773e+00\n",
      "  -9.93397936e-01  4.22669750e-15]\n",
      " [-1.40499448e+00  9.78719965e-02 -3.45343680e-01 ... -5.10762097e-01\n",
      "  -1.66803272e+00  4.22669750e-15]\n",
      " ...\n",
      " [ 4.85000369e+00 -4.85771050e+00  3.79229550e+00 ...  1.03772523e-01\n",
      "  -2.01921434e-01  4.22669750e-15]\n",
      " [-1.22662864e+00  2.81809936e-01 -4.41758707e-01 ...  1.02968771e+00\n",
      "  -1.02025218e-01  4.22669750e-15]\n",
      " [-1.41125238e+00  3.94713582e-01 -2.02570439e-01 ...  1.17276673e+00\n",
      "   1.51261029e-01  4.22669750e-15]]\n"
     ]
    }
   ],
   "source": [
    "# 当有本地csv数据时，使用以下方法获取数据\n",
    "def csvLoader(csvpath):\n",
    "    csv_data = pd.read_csv(csvpath,header=0)\n",
    "    #print(csv_data)\n",
    "    # 转换为numpy矩阵\n",
    "    return np.array(csv_data)\n",
    "\n",
    "PCA_projected_trainData = csvLoader('PCAProjectedTrainData800.csv')\n",
    "PCA_projected_testData = csvLoader('PCAProjectedTestData800.csv')\n",
    "#print(PCA_projected_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '1' '0' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0' '1'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '1' '1' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '1' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
      " '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "train_labels=[]\n",
    "for c in getListFromFile('dorothea_train.labels'):\n",
    "    if c!= '1':\n",
    "        train_labels.append('0')\n",
    "    else:train_labels.append(c)\n",
    "train_labels=np.asarray(train_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, recall_score\n",
    "#Read the input files and read every line\n",
    "def loadData(trainingFile, testingFile):\n",
    "    \n",
    "    def convertDataframe(inputFile):\n",
    "        data = pd.DataFrame(columns=range(100000))\n",
    "        \n",
    "        for i in range(len(inputFile)):\n",
    "            record = np.fromstring(inputFile[i], dtype=int, sep=' ')\n",
    "            record_bool = [0 for j in range(100000)]\n",
    "            for col in record:\n",
    "                record_bool[col-1] = 1\n",
    "            \n",
    "            data.loc[i] = record_bool\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    with open(trainingFile, \"r\") as fr1:\n",
    "        trainFile = fr1.readlines()\n",
    "    \n",
    "    #Split each line in the two files into label and data  \n",
    "    train_data_list = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    for inputData in trainFile:\n",
    "        train_labels_list.append(inputData[0])\n",
    "        #Remove the activity label (0/1) and new line character from each record\n",
    "        inputData = inputData.replace(\"0\\t\", \"\")\n",
    "        inputData = inputData.replace(\"1\\t\", \"\")\n",
    "        inputData = inputData.replace(\"\\n\", \"\")\n",
    "        train_data_list.append(inputData)\n",
    "    \n",
    "    train_labels = np.asarray(train_labels_list)\n",
    "    train_data = convertDataframe(train_data_list)\n",
    "        \n",
    "    with open(testingFile, \"r\") as fr2:\n",
    "        testFile = fr2.readlines()\n",
    "    \n",
    "    test_data = convertDataframe(testFile)\n",
    "            \n",
    "    return train_data, test_data, train_labels\n",
    "\n",
    "# Project data on a reduced dimensionality k using PCA\n",
    "def pca(train_data, test_data, k=500):\n",
    "\n",
    "    pca = sklearnPCA(n_components = k)\n",
    "    PCA_projected_trainData = pca.fit_transform(train_data)\n",
    "    PCA_projected_testData = pca.transform(test_data)\n",
    "    \n",
    "    return PCA_projected_trainData, PCA_projected_testData\n",
    "\n",
    "# Save data handled by PCA\n",
    "def pcaData2csv(csvpath1,csvpath2, train_data,test_data):\n",
    "    dftrain=pd.DataFrame(train_data)\n",
    "    dftest=pd.DataFrame(test_data)\n",
    "    dftrain.to_csv(csvpath1,index=False,sep=',')\n",
    "    dftest.to_csv(csvpath2,index=False,sep=',')\n",
    "    if os.path.exists(csvpath1):\n",
    "        print(\"PCA projected train data has been written into csv files in:\"+str(csvpath1))\n",
    "        if os.path.exists(csvpath2):\n",
    "            print(\"PCA projected test data has been written into csv files in:\"+str(csvpath2))\n",
    "        else:\n",
    "            raise Warning(\"File written failed in:\"+str(csvpath2))\n",
    "    else:raise Warning(\"File written failed in:\"+str(csvpath1))\n",
    "    # 抛出dataframe类型以备其他用途\n",
    "    return dftrain,dftest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "主成分分析（Principal Components Analysis），简称PCA，是一种数据降维技术，用于数据预处理。\n",
    "\n",
    "PCA的一般步骤是：先对原始数据零均值化，然后求协方差矩阵，接着对协方差矩阵求特征向量和特征值，这些特征向量组成了新的特征空间。\n",
    "\n",
    "sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)\n",
    "\n",
    "## 参数：\n",
    "\n",
    "n_components:  \n",
    "\n",
    "意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n\n",
    "\n",
    "类型：int 或者 string，缺省时默认为None，所有成分被保留。\n",
    "\n",
    "          赋值为int，比如n_components=1，将把原始数据降到一个维度。\n",
    "\n",
    "          赋值为string，比如n_components='mle'，将自动选取特征个数n，使得满足所要求的方差百分比。\n",
    "\n",
    "copy:\n",
    "\n",
    "类型：bool，True或者False，缺省时默认为True。\n",
    "意义：表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不            会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的              值会改，因为是在原始数据上进行降维计算。\n",
    "\n",
    "whiten:\n",
    "\n",
    "类型：bool，缺省时默认为False\n",
    "\n",
    "意义：白化，使得每个特征具有相同的方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the training and the test data set and get 3 separate dataframes of training reviews, test reviews and training labels\n",
    "train_data, test_data, train_labels = loadData('train.dat', 'test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the number of dimensions from 100000 to 500 using PCA\n",
    "PCA_projected_trainData, PCA_projected_testData = pca(train_data, test_data, 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA projected train data has been written into csv files in:PCAProjectedTrainData800.csv\n",
      "PCA projected test data has been written into csv files in:PCAProjectedTestData800.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          0         1         2         3         4         5         6    \\\n",
       " 0   -1.125879 -0.128395 -0.102923 -0.313411  0.160987  0.130299  0.063675   \n",
       " 1   -1.308421  0.193540 -0.405118 -0.140494 -0.031587  0.171552 -0.039403   \n",
       " 2   -1.404994  0.097872 -0.345344 -0.158859  0.093332  0.388095  0.126652   \n",
       " 3   -1.361430  0.190956 -0.230127 -0.239852 -0.099705  0.563770 -0.133497   \n",
       " 4   -1.092508  0.367352 -0.308897 -0.146139  0.008681  0.363045  0.103659   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 795 -1.338668  0.192657 -0.293322 -0.211951  0.117938  0.274550  0.059335   \n",
       " 796 -1.295912  0.028073 -0.169278 -0.241292 -0.034312  0.233793 -0.211511   \n",
       " 797  4.850004 -4.857711  3.792295 -0.416099  0.330068  1.029004 -1.062087   \n",
       " 798 -1.226629  0.281810 -0.441759 -0.202731  0.130665  0.233283  0.080688   \n",
       " 799 -1.411252  0.394714 -0.202570 -0.235522 -0.020797  0.280551  0.390161   \n",
       " \n",
       "           7         8         9    ...       790       791       792  \\\n",
       " 0    0.450288 -0.157234  0.096230  ...  0.591679 -0.433868  0.330060   \n",
       " 1    0.031781 -0.042687 -0.113722  ... -2.500523 -0.068946  0.233799   \n",
       " 2    0.165808 -0.136963  0.054194  ... -0.733850  0.125104  0.154526   \n",
       " 3   -0.001468  0.077094 -0.264311  ... -2.449887 -0.045121  0.949932   \n",
       " 4    0.035156 -0.103026 -0.185720  ...  0.090070  0.997314  0.455775   \n",
       " ..        ...       ...       ...  ...       ...       ...       ...   \n",
       " 795  0.418259  0.041917 -0.248520  ...  0.535063 -2.228013 -1.975827   \n",
       " 796  0.203369 -0.457087  0.535415  ...  0.719330  1.306541  0.230879   \n",
       " 797 -0.623489 -0.295882  0.648500  ...  0.546472  0.464372 -0.139380   \n",
       " 798  0.254147 -0.111326 -0.037910  ...  0.928173  1.194231  0.225038   \n",
       " 799 -0.087616  0.246312 -0.071702  ... -0.589992 -0.247325  0.650219   \n",
       " \n",
       "           793       794       795       796       797       798           799  \n",
       " 0   -1.678149  0.094964  0.289267  0.029416 -1.079556  1.075511  4.226697e-15  \n",
       " 1   -0.083210  1.660202 -0.642811 -1.094371 -1.561658 -0.993398  4.226697e-15  \n",
       " 2    1.446838  0.079466 -1.072789 -0.758261 -0.510762 -1.668033  4.226697e-15  \n",
       " 3    0.862832 -1.077406 -1.125775 -1.257380  0.084179 -1.038416  4.226697e-15  \n",
       " 4    0.430445 -0.245760 -0.134196  0.080085 -0.011085  0.576773  4.226697e-15  \n",
       " ..        ...       ...       ...       ...       ...       ...           ...  \n",
       " 795  0.384475  0.750554  0.389151 -0.417789 -2.031564 -1.030647  4.226697e-15  \n",
       " 796 -0.177127 -0.628271 -0.942699  0.450452 -0.627904 -1.713722  4.226697e-15  \n",
       " 797  0.245321 -0.163991  0.422651 -0.576583  0.103773 -0.201921  4.226697e-15  \n",
       " 798  0.716329 -0.234404 -0.144372  0.433919  1.029688 -0.102025  4.226697e-15  \n",
       " 799 -0.952441  0.423251  0.688583 -0.985653  1.172767  0.151261  4.226697e-15  \n",
       " \n",
       " [800 rows x 800 columns],\n",
       "           0         1         2         3         4         5         6    \\\n",
       " 0   -0.585532 -0.242962 -0.099596  0.038881 -0.088539 -0.092319  0.110493   \n",
       " 1   -1.153384  0.058960 -0.123457 -0.040103  0.006709  0.145189  0.093507   \n",
       " 2   -0.743941  0.785020  0.609501  0.110092 -0.000681  0.003134  1.509105   \n",
       " 3   -1.020475  0.044454 -0.109064 -0.119680  0.203390  0.181658 -0.117516   \n",
       " 4   -0.738698 -0.228193  0.011234 -0.178726  0.125716  0.244385 -0.139428   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 345 -1.125824  0.055316 -0.253516 -0.136528  0.119428  0.362042  0.027249   \n",
       " 346 -1.176939  0.207874 -0.249694 -0.104565 -0.013160  0.321256 -0.029438   \n",
       " 347 -0.974201  0.209110 -0.457702 -0.197669 -0.034474 -0.109695 -0.224478   \n",
       " 348 -1.121436  0.739078  0.269613  0.182792 -0.045546  0.222943  0.164051   \n",
       " 349 -1.278630  0.067160 -0.271880 -0.131846  0.054076  0.246099 -0.051180   \n",
       " \n",
       "           7         8         9    ...       790       791       792  \\\n",
       " 0    1.034946 -0.116975  0.456917  ...  0.097502 -0.097847 -0.227756   \n",
       " 1    0.153683 -0.047909 -0.087731  ...  0.122400  0.064427 -0.170240   \n",
       " 2   -0.711560  0.050279  0.056249  ...  0.220881  0.006823  0.010978   \n",
       " 3   -0.031326 -0.216032 -0.022563  ...  0.172632 -0.263595 -0.071231   \n",
       " 4    0.035747 -0.297232  0.101219  ... -0.204667  0.255948  0.182476   \n",
       " ..        ...       ...       ...  ...       ...       ...       ...   \n",
       " 345  0.193960  0.068820  0.016842  ...  0.052816 -0.104336  0.081834   \n",
       " 346  0.114614  0.044067 -0.160899  ... -0.195177  0.208070 -0.118211   \n",
       " 347  0.098366 -0.110753  0.067856  ...  0.052282 -0.231843  0.003767   \n",
       " 348 -0.060680 -0.044601 -0.015458  ...  0.045782 -0.100845  0.055029   \n",
       " 349  0.192499 -0.278679  0.099388  ...  0.247109 -0.001702 -0.041142   \n",
       " \n",
       "           793       794       795       796       797       798       799  \n",
       " 0   -0.046042 -0.078609  0.109323  0.202760  0.190586 -0.093135  0.060937  \n",
       " 1    0.018230  0.230232  0.093306  0.071759 -0.381283 -0.097952  0.104451  \n",
       " 2   -0.066370  0.018048 -0.140462 -0.109699 -0.036796  0.192773  0.014904  \n",
       " 3   -0.023384  0.010761 -0.043245  0.006015  0.091292  0.025407 -0.051858  \n",
       " 4    0.065516  0.065678 -0.117101  0.033468  0.255777  0.131605 -0.014507  \n",
       " ..        ...       ...       ...       ...       ...       ...       ...  \n",
       " 345 -0.081960 -0.075441 -0.062177 -0.045129 -0.186388 -0.103952  0.045987  \n",
       " 346  0.047809  0.108492 -0.122903 -0.052362 -0.178814 -0.079602  0.044109  \n",
       " 347 -0.221987  0.100995  0.118445 -0.053357  0.028730 -0.226392  0.015026  \n",
       " 348 -0.316385  0.077476  0.001343  0.139585  0.078801  0.197050 -0.159743  \n",
       " 349  0.167886 -0.086324  0.055463 -0.178630 -0.048692  0.053140 -0.071314  \n",
       " \n",
       " [350 rows x 800 columns])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path1=r\"PCAProjectedTrainData800.csv\"\n",
    "csv_path2=r\"PCAProjectedTestData800.csv\"\n",
    "pcaData2csv(csv_path1,csv_path2, PCA_projected_trainData,PCA_projected_testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 伯努利朴素贝叶斯BernoulliNB\n",
    "\n",
    "多项式朴素贝叶斯可同时处理二项分布（抛硬币）和多项分布（掷骰子），其中二项分布又叫做伯努利分布，它是一种现实中常见，并且拥有很多优越数学性质的分布。因此，既然有着多项式朴素贝叶斯，我们自然也就又专门用来处理二项分布的朴素贝叶斯：伯努利朴素贝叶斯。\n",
    "\n",
    "伯努利贝叶斯类BernoulliN假设数据服从多元伯努利分布，并在此基础上应用朴素贝叶斯的训练和分类过程。多元伯努利分布简单来说，就是数据集中可以存在多个特征，但每个特征都是二分类的，可以以布尔变量表示，也可以表示为{0，1}或者{-1，1}等任意二分类组合。因此，这个类要求将样本转换为二分类特征向量，如果数据本身不是二分类的，那可以使用类中专门用来二值化的参数binarize来改变数据。\n",
    "\n",
    "伯努利朴素贝叶斯与多项式朴素贝叶斯非常相似，都常用于处理文本分类数据。但由于伯努利朴素贝叶斯是处理二项分布，所以它更加在意的是“存在与否”，而不是“出现多少次”这样的次数或频率，这是伯努利贝叶斯与多项式贝叶斯的根本性不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, average_precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def resultAnalysis(testData,testLabels,pred,proba,_module=None):\n",
    "    xmodulelist = ['macro','micro','weighted','non-avg']\n",
    "    criteion = ['Accuracy','Precision','Recall','F1_score','AUC','AUPRC']\n",
    "    df = pd.DataFrame(index=xmodulelist,columns=criteion)\n",
    "    \n",
    "    \n",
    "    if _module not in xmodulelist:\n",
    "        \n",
    "        df['Accuracy']['non-avg']=round(accuracy_score(testLabels,pred),3)\n",
    "        df['Precision']['non-avg']=round(precision_score(testLabels,pred,),3)\n",
    "        df['Recall']['non-avg']=round(recall_score(testLabels,pred,),3)\n",
    "        df['AUC']['non-avg']=round(roc_auc_score(testLabels,proba,),3)\n",
    "        df['AUPRC']['non-avg']=round(average_precision_score(testLabels,pred,),3)\n",
    "        df['F1_score']['non-avg']=round(f1_score(testLabels,pred,),3)\n",
    "        print(df.iloc[3,:])\n",
    "            \n",
    "    else:\n",
    "        df['Accuracy'][_module]=round(accuracy_score(testLabels,pred),3)\n",
    "        df['Precision'][_module]=round(precision_score(testLabels,pred,average=_module),3)\n",
    "        df['Recall'][_module]=round(recall_score(testLabels,pred,average=_module),3)\n",
    "        df['AUC'][_module]=round(roc_auc_score(testLabels,proba,average=_module),3)\n",
    "        df['AUPRC'][_module]=round(average_precision_score(testLabels,pred,average=_module),3)\n",
    "        df['F1_score'][_module]=round(f1_score(testLabels,pred,average=_module),3)\n",
    "        print(df.loc[_module,:])\n",
    "    print(confusion_matrix(testLabels,pred))\n",
    "    return classification_report(testLabels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(trainingFile, \"r\") as fr1:\n",
    "#    trainFile = fr1.readlines()\n",
    "def getListFromFile(_filename):\n",
    "    _list = []\n",
    "    with open(_filename, \"r\") as fr3:\n",
    "        dat_r = fr3.readlines()\n",
    "    for inputData in dat_r:\n",
    "        _list.append(inputData[0])\n",
    "    target = np.asarray(_list)\n",
    "    return target\n",
    "\n",
    "test_labels = getListFromFile('valid_labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.929\n",
      "Precision    0.645\n",
      "Recall       0.588\n",
      "F1_score     0.615\n",
      "AUC          0.882\n",
      "AUPRC         0.42\n",
      "Name: non-avg, dtype: object\n",
      "[[305  11]\n",
      " [ 14  20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       316\n",
      "           1       0.65      0.59      0.62        34\n",
      "\n",
      "    accuracy                           0.93       350\n",
      "   macro avg       0.80      0.78      0.79       350\n",
      "weighted avg       0.93      0.93      0.93       350\n",
      "\n",
      "\tAccuracy:0.929\n",
      "\tPrecision:0.645\n",
      "\tRecall:0.588\n",
      "\tF1_score:0.615\n",
      "\tAUC:0.882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def BNBC2(trainData,trainLabels, testData,testLabels):\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    clf = BernoulliNB (alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "    clf.fit(trainData,trainLabels)\n",
    "    \n",
    "    pred = clf.predict(testData)\n",
    "    proba = clf.predict_proba(testData)[:,1]\n",
    "    score = clf.score(testData,testLabels)\n",
    "    fpr, tpr, thresholds = roc_curve(testLabels, proba)\n",
    "    rocauc=auc(fpr, tpr)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    #print(proba)\n",
    "    print(\"\\tAccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\tPrecision:{:.3f}\".format(precision_score(testLabels,pred)))\n",
    "    print(\"\\tRecall:{:.3f}\".format(recall_score(testLabels,pred)))\n",
    "    print(\"\\tF1_score:{:.3f}\".format(f1_score(testLabels,pred)))\n",
    "    print(\"\\tAUC:{:.3f}\".format(roc_auc_score(testLabels,proba)))\n",
    "    return pred\n",
    "\n",
    "BNBC2(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.886\n",
      "Precision    0.442\n",
      "Recall       0.676\n",
      "F1_score     0.535\n",
      "AUC           0.81\n",
      "AUPRC        0.331\n",
      "Name: non-avg, dtype: object\n",
      "[[287  29]\n",
      " [ 11  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       316\n",
      "           1       0.44      0.68      0.53        34\n",
      "\n",
      "    accuracy                           0.89       350\n",
      "   macro avg       0.70      0.79      0.73       350\n",
      "weighted avg       0.91      0.89      0.90       350\n",
      "\n",
      "\tAccuracy:0.886\n",
      "\tPrecision:0.442\n",
      "\tRecall:0.676\n",
      "\tAUC:0.810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "def min_max_normalization(np_array):\n",
    "    min_max_scaler = MinMaxScaler(feature_range=[0,10])\n",
    "    ret = min_max_scaler.fit_transform(np_array)\n",
    "    return ret\n",
    "\n",
    "def CNB(trainData,trainLabels, testData,testLabels):\n",
    "\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    trainData=min_max_normalization(trainData)\n",
    "    testData=min_max_normalization(testData)\n",
    "    \n",
    "    clf = ComplementNB(alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
    "    clf.fit(trainData,trainLabels)\n",
    "\n",
    "    pred = clf.predict(testData)\n",
    "    proba = clf.predict_proba(testData)[:,1]\n",
    "    score = clf.score(testData,testLabels)\n",
    "    fpr, tpr, thresholds = roc_curve(testLabels, proba)\n",
    "    #rocauc=auc(fpr, tpr)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    # 以下针对少数类\n",
    "    print(\"\\tAccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\tPrecision:{:.3f}\".format(precision_score(testLabels,pred)))\n",
    "    print(\"\\tRecall:{:.3f}\".format(recall_score(testLabels,pred)))\n",
    "    print(\"\\tAUC:{:.3f}\".format(roc_auc_score(testLabels,proba)))\n",
    "    return pred\n",
    "\n",
    "CNB(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy     0.903\n",
      "Precision      0.0\n",
      "Recall         0.0\n",
      "F1_score       0.0\n",
      "AUC            0.5\n",
      "AUPRC        0.097\n",
      "Name: non-avg, dtype: object\n",
      "[[316   0]\n",
      " [ 34   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       316\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.90       350\n",
      "   macro avg       0.45      0.50      0.47       350\n",
      "weighted avg       0.82      0.90      0.86       350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNNC(trainData,trainLabels, testData,testLabels,k):\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,weights='distance')\n",
    "    \n",
    "    knn.fit(trainData,trainLabels)\n",
    "    \n",
    "    pred = knn.predict(testData)\n",
    "    proba = knn.predict_proba(testData)[:,1]\n",
    "    print(proba)\n",
    "    score = knn.score(testData,testLabels)\n",
    "    fpr, tpr, thresholds = roc_curve(testLabels, proba)\n",
    "    rocauc=auc(fpr, tpr)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    return pred\n",
    "\n",
    "KNNC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,3)\n",
    "# KNN不适合不平衡数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    #num = min(len(_preds),len(_labels))\n",
    "    #print(num)\n",
    "    labels=labels.astype(int)\n",
    "    preds=preds.astype(int)\n",
    "    num=len(preds)\n",
    "    #print(preds)\n",
    "    correct = 0\n",
    "    print(preds[:10])\n",
    "    print(labels[:10])\n",
    "    tp = 0 #true_positives\n",
    "    tn = 0 #true_negatives\n",
    "    fp = 0 #false_positives\n",
    "    fn = 0 #false_negatives\n",
    "    for i in range(num):\n",
    "        if preds[i]==labels[i]:\n",
    "            correct+=1\n",
    "            if preds[i]:\n",
    "                tp+=1\n",
    "            else:tn+=1\n",
    "        else:\n",
    "            if preds[i]:\n",
    "                fp+=1\n",
    "            else:fn+=1\n",
    "    print(\"预测总数:%s\" % len(preds))\n",
    "    print(\"标签数:%s\" % len(labels))\n",
    "    print(\"正确数:%s\" % correct)\n",
    "    print(\"TP:FP\",(tp,fp))\n",
    "    print(\"TN:FN\",(tn,fn))\n",
    "    print(\"Recall\",recall_score(labels.astype(int),preds.astype(int),pos_label=1))\n",
    "    _acc = correct/len(labels)\n",
    "    _prec = tp/(tp+fp)\n",
    "    _recall = tp/(tp+fn)\n",
    "    _fpr = fp/(fp+tn)\n",
    "    _tnr = tn/(fp+tn)\n",
    "    _f1 = 2*_prec*_recall/(_prec+_recall)\n",
    "    return({'ACC':_acc,'Precision':_prec,'Recall':_recall,'FPR':_fpr,'TNR':_tnr,'F1':_f1})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy：准确率\n",
    "\n",
    "也就是所有预测正确的和所有test集的比例。\n",
    "\n",
    "准确率=预测正确的样本数/所有样本数，即预测正确的样本比例（包括预测正确的正样本和预测正确的负样本）。\n",
    "\n",
    ">Accuracy = T/(T+F)\n",
    "\n",
    "### Precision：查准率\n",
    "\n",
    ">Precision=TP/(TP+FP)\n",
    "\n",
    "用于衡量模型对某一类的预测有多准。\n",
    "\n",
    "### Recall：召回率（真正类率）\n",
    "\n",
    ">Recall = TP/(TP+FN)\n",
    "\n",
    "指的是某个类别的Recall。Recall表示某一类样本，预测正确的与所有Ground Truth的比例。\n",
    "\n",
    "### FPR\n",
    "\n",
    ">FPR = FP/(TN+FP)\n",
    "\n",
    "代表分类器预测的正类中实际负实例占所有负实例的比例。\n",
    ">FPR = 1 - TNR\n",
    "\n",
    "### TNR\n",
    "\n",
    ">TNR = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel linear prediction is:\n",
      "Accuracy     0.931\n",
      "Precision    0.917\n",
      "Recall       0.324\n",
      "F1_score     0.478\n",
      "AUC          0.917\n",
      "AUPRC        0.362\n",
      "Name: non-avg, dtype: object\n",
      "[[315   1]\n",
      " [ 23  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       316\n",
      "           1       0.92      0.32      0.48        34\n",
      "\n",
      "    accuracy                           0.93       350\n",
      "   macro avg       0.92      0.66      0.72       350\n",
      "weighted avg       0.93      0.93      0.92       350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel rbf prediction is:\n",
      "Accuracy     0.903\n",
      "Precision      0.0\n",
      "Recall         0.0\n",
      "F1_score       0.0\n",
      "AUC          0.925\n",
      "AUPRC        0.097\n",
      "Name: non-avg, dtype: object\n",
      "[[316   0]\n",
      " [ 34   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       316\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.90       350\n",
      "   macro avg       0.45      0.50      0.47       350\n",
      "weighted avg       0.82      0.90      0.86       350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel poly prediction is:\n",
      "Accuracy     0.903\n",
      "Precision      0.0\n",
      "Recall         0.0\n",
      "F1_score       0.0\n",
      "AUC          0.926\n",
      "AUPRC        0.097\n",
      "Name: non-avg, dtype: object\n",
      "[[316   0]\n",
      " [ 34   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       316\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.90       350\n",
      "   macro avg       0.45      0.50      0.47       350\n",
      "weighted avg       0.82      0.90      0.86       350\n",
      "\n",
      "The kernel sigmoid prediction is:\n",
      "Accuracy     0.923\n",
      "Precision    0.889\n",
      "Recall       0.235\n",
      "F1_score     0.372\n",
      "AUC          0.925\n",
      "AUPRC        0.283\n",
      "Name: non-avg, dtype: object\n",
      "[[315   1]\n",
      " [ 26   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       316\n",
      "           1       0.89      0.24      0.37        34\n",
      "\n",
      "    accuracy                           0.92       350\n",
      "   macro avg       0.91      0.62      0.67       350\n",
      "weighted avg       0.92      0.92      0.90       350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn import svm\n",
    "#from sklearn.metrics import roc_curve, roc_auc_score, recall_score, precision_score\n",
    "#from imblearn import specificity_score\n",
    "\n",
    "def SVMC_kernel(trainData,trainLabels, testData,testLabels, weights='balanced'):\n",
    "    #clf = svm.SVC(gamma='scale', class_weight=weights)\n",
    "    #clf.fit(trainData,trainLabels)\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    for k in ['linear','rbf','poly','sigmoid']:\n",
    "        clf_proba = svm.SVC(kernel=k,C=1.0,probability=True).fit(trainData,trainLabels)\n",
    "        #print(clf_proba.decision_function(testData).shape)\n",
    "        #predictions = clf.predict(testData)\n",
    "        clf_pred = clf_proba.predict(testData)\n",
    "        result = clf_proba.score(testData, testLabels)\n",
    "        score = clf_proba.decision_function(testData)\n",
    "\n",
    "        rocauc = roc_auc_score(testLabels, score)\n",
    "        recall = recall_score(testLabels, clf_pred)\n",
    "        precision = precision_score(testLabels, clf_pred)\n",
    "        print(\"The kernel {} prediction is:\".format(k))\n",
    "        print(resultAnalysis(testData,testLabels,clf_pred,score,))\n",
    "    #print(\"\\tAccuracy:{:.3f}\".format(result))\n",
    "    #print(\"\\tPrecision:{:.3f}\".format(precision))\n",
    "    #print(\"\\tRecall:{:.3f}\".format(recall))\n",
    "    #print(\"\\tSpecificity:{:.3f}\".format(specificity_score(testLabels, clf_pred)))\n",
    "    #print(\"\\tAUC:{:.3f}\".format(rocauc))\n",
    "    #return clf_pred\n",
    "\n",
    "SVMC_kernel(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.914\n",
      "Precision      1.0\n",
      "Recall       0.118\n",
      "F1_score     0.211\n",
      "AUC           0.92\n",
      "AUPRC        0.203\n",
      "Name: non-avg, dtype: object\n",
      "[[316   0]\n",
      " [ 30   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       316\n",
      "           1       1.00      0.12      0.21        34\n",
      "\n",
      "    accuracy                           0.91       350\n",
      "   macro avg       0.96      0.56      0.58       350\n",
      "weighted avg       0.92      0.91      0.88       350\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SVMC(trainData,trainLabels, testData,testLabels,weights='balanced'):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    clf_proba = svm.SVC(gamma='scale', class_weight=weights).fit(trainData,trainLabels)\n",
    "    #print(clf_proba.decision_function(testData).shape)\n",
    "    #predictions = clf.predict(testData)\n",
    "    \n",
    "    clf_pred = clf_proba.predict(testData)\n",
    "    \n",
    "    result = clf_proba.score(testData, testLabels)\n",
    "    score = clf_proba.decision_function(testData)\n",
    "    \n",
    "    rocauc = roc_auc_score(testLabels, score)\n",
    "    recall = recall_score(testLabels, clf_pred)\n",
    "    precision = precision_score(testLabels, clf_pred)\n",
    "    \n",
    "    print(resultAnalysis(testData,testLabels,clf_pred,score,))\n",
    "    #print(\"\\tAccuracy:{:.3f}\".format(result))\n",
    "    #print(\"\\tPrecision:{:.3f}\".format(precision))\n",
    "    #print(\"\\tRecall:{:.3f}\".format(recall))\n",
    "    #print(\"\\tSpecificity:{:.3f}\".format(specificity_score(testLabels, clf_pred)))\n",
    "    #print(\"\\tAUC:{:.3f}\".format(rocauc))\n",
    "    return clf_pred\n",
    "SVMC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "预测总数:350\n",
      "标签数:350\n",
      "正确数:329\n",
      "TP:FP (20, 7)\n",
      "TN:FN (309, 14)\n",
      "Recall 0.5882352941176471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': 0.94,\n",
       " 'Precision': 0.7407407407407407,\n",
       " 'Recall': 0.5882352941176471,\n",
       " 'FPR': 0.022151898734177215,\n",
       " 'TNR': 0.9778481012658228,\n",
       " 'F1': 0.6557377049180328}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#朴素贝叶斯分类器\n",
    "bnbc_preds = getListFromFile('output-k-100-PCA-BNBC.dat')\n",
    "accuracy(bnbc_preds,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "预测总数:350\n",
      "标签数:350\n",
      "正确数:325\n",
      "TP:FP (10, 1)\n",
      "TN:FN (315, 24)\n",
      "Recall 0.29411764705882354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': 0.9285714285714286,\n",
       " 'Precision': 0.9090909090909091,\n",
       " 'Recall': 0.29411764705882354,\n",
       " 'FPR': 0.0031645569620253164,\n",
       " 'TNR': 0.9968354430379747,\n",
       " 'F1': 0.4444444444444445}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "svm_preds = getListFromFile('output-k-100-PCA-SVM.dat')\n",
    "accuracy(svm_preds,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "预测总数:350\n",
      "标签数:350\n",
      "正确数:329\n",
      "TP:FP (18, 5)\n",
      "TN:FN (311, 16)\n",
      "Recall 0.5294117647058824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': 0.94,\n",
       " 'Precision': 0.782608695652174,\n",
       " 'Recall': 0.5294117647058824,\n",
       " 'FPR': 0.015822784810126583,\n",
       " 'TNR': 0.9841772151898734,\n",
       " 'F1': 0.631578947368421}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "#test_labels = getListFromFile('valid_labels.dat')\n",
    "rfc_preds = getListFromFile('output-k-100-PCA-RFC.dat')\n",
    "accuracy(rfc_preds, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.926\n",
      "Precision      0.9\n",
      "Recall       0.265\n",
      "F1_score     0.409\n",
      "AUC          0.915\n",
      "AUPRC         0.31\n",
      "Name: non-avg, dtype: object\n",
      "[[315   1]\n",
      " [ 25   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       316\n",
      "           1       0.90      0.26      0.41        34\n",
      "\n",
      "    accuracy                           0.93       350\n",
      "   macro avg       0.91      0.63      0.68       350\n",
      "weighted avg       0.92      0.93      0.91       350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-k-100-PCA-SVM.dat written completed.\n"
     ]
    }
   ],
   "source": [
    "#svm_preds = SVMC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)\n",
    "getPredictionFile('output-k-100-PCA-SVM.dat',svm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.929\n",
      "Precision    0.909\n",
      "Recall       0.294\n",
      "F1_score     0.444\n",
      "AUC          0.927\n",
      "AUPRC        0.336\n",
      "Name: non-avg, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       316\n",
      "           1       0.91      0.29      0.44        34\n",
      "\n",
      "    accuracy                           0.93       350\n",
      "   macro avg       0.92      0.65      0.70       350\n",
      "weighted avg       0.93      0.93      0.91       350\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "预测总数:350\n",
      "标签数:350\n",
      "正确数:325\n",
      "TP:FP (10, 1)\n",
      "TN:FN (315, 24)\n",
      "Recall 0.29411764705882354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': 0.9285714285714286,\n",
       " 'Precision': 0.9090909090909091,\n",
       " 'Recall': 0.29411764705882354,\n",
       " 'FPR': 0.0031645569620253164,\n",
       " 'TNR': 0.9968354430379747,\n",
       " 'F1': 0.4444444444444445}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.metrics import roc_curve,precision_recall_curve,roc_auc_score,average_precision_score,f1_score\n",
    "#test_labels = getListFromFile('valid_labels.dat')\n",
    "weights = {'1':34,'0':316}\n",
    "\n",
    "svm_preds2=SVMC2(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,weights=weights)\n",
    "#svm_f1 = f1_score(list(map(int,test_labels)),svm_score)\n",
    "#print('F1:',svm_f1)\n",
    "\n",
    "#svm_preds = getListFromFile('output-k-100-PCA-SVM.dat')\n",
    "accuracy(svm_preds2,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Data = np.vstack((PCA_projected_trainData,PCA_projected_testData))\n",
    "all_labels = np.hstack((train_labels,test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.949\n",
      "Precision    0.735\n",
      "Recall       0.735\n",
      "F1_score     0.735\n",
      "AUC          0.925\n",
      "AUPRC        0.566\n",
      "Name: non-avg, dtype: object\n",
      "[[307   9]\n",
      " [  9  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       316\n",
      "           1       0.74      0.74      0.74        34\n",
      "\n",
      "    accuracy                           0.95       350\n",
      "   macro avg       0.85      0.85      0.85       350\n",
      "weighted avg       0.95      0.95      0.95       350\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier     #随机森林\n",
    "\n",
    "def RFC(trainData,trainLabels, testData,testLabels):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=63, max_depth=20,max_features=26,min_samples_leaf=3,min_samples_split=14,criterion='entropy',\n",
    "                                 class_weight='balanced',random_state=42)                      #不调参\n",
    "    rfc = rfc.fit(trainData, trainLabels)                 #用训练集数据训练模型\n",
    "    pred = rfc.predict(testData)\n",
    "    result = rfc.score(testData, testLabels)\n",
    "    proba = rfc.predict_proba(testData)[:,1]\n",
    "    rocauc = roc_auc_score(testLabels, proba)\n",
    "    recall = recall_score(testLabels, pred)\n",
    "    precision = precision_score(testLabels, pred)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    \n",
    "    return pred\n",
    "RFC(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getROCFig(testLabels,score):\n",
    "    fpr,tpr,thresholds = roc_curve(testLabels,score)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.3f)' % rocauc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost可以通过使用一些特定参数来实现随机森林算法中的bagging（bootstrap aggregating）方法，在集成几个弱分类器的同时，构建一个更强的分类器。具体实现方法是在构建每个树的时候随机选择一部分特征进行分裂。\n",
    "\n",
    "针对xgboost中的不平衡分类问题，可以通过以下参数进行调参：\n",
    "\n",
    "* scale_pos_weight: 用于调整正负样本的权重，通常情况下，正负样本数目差异比较大时需要设置该参数，将其设置为负样本数/正样本数，以达到平衡正负样本的权重。\n",
    "* subsample 和 colsample_bytree: 这两个参数可以控制树的生长和特征选择的随机性，通过调节这两个参数可以缓解过拟合的问题，从而提高模型的分类能力和泛化性能。\n",
    "* max_depth 和 min_child_weight: 这两个参数可以控制树的复杂度和节点的最小权重，用于防止过度拟合和过度生长树的问题。\n",
    "* gamma: 用于指定节点划分所需要的最小loss减少量，可以用于控制决策树结构的复杂度，从而改善模型在验证集上的分类性能。\n",
    "* eta: 也被称为学习率或者步长，控制模型对于每个树学习到的分数的缩放比例，太大的学习率会导致模型发散，太小的学习率则会使得模型收敛缓慢。\n",
    "    \n",
    "在调参时，可以使用网格搜索或者随机搜索等方法搭配交叉验证来选择最优的参数组合，同时也可以通过可视化学习曲线和特征重要性等指标来对调参结果进行评估和选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大得分：0.8915844875189535 子树数量为：121\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 调参，绘制学习曲线来调参n_estimators（对随机森林影响最大）\n",
    "score_lt = []\n",
    "\n",
    "# 每隔10步建立一个随机森林，获得不同n_estimators的得分\n",
    "for i in range(50,200,10):\n",
    "    rfc = RandomForestClassifier(n_estimators=i+1,random_state=42)\n",
    "    score = cross_val_score(rfc, PCA_Data, all_labels.astype(int), cv=10,scoring='roc_auc').mean()\n",
    "    score_lt.append(score)\n",
    "score_max = max(score_lt)\n",
    "print('最大得分：{}'.format(score_max),\n",
    "      '子树数量为：{}'.format(score_lt.index(score_max)*10+1))\n",
    "\n",
    "# 绘制学习曲线\n",
    "x = np.arange(51,201,10)\n",
    "plt.subplot(111)\n",
    "plt.plot(x, score_lt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.metrics.roc_auc_score()的使用方法\n",
    "\n",
    "`sklearn.metrics.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)`\n",
    "\n",
    "* 参数说明\n",
    "\n",
    "+ y_true：真实的标签。形状(n_samples，)或(n_samples, n_classes)。二分类的形状(n_samples，1)，而多标签情况的形状(n_samples, n_classes)。\n",
    "\n",
    "+ y_score：目标分数。形状(n_samples，)或(n_samples, n_classes)。二分类情况形状(n_samples，1)，“分数必须是具有较大标签的类的分数”，通俗点理解:模型打分的第二列。举个例子:模型输入的得分是一个数组[0.98361117 0.01638886]，索引是其类别，这里“较大标签类的分数”，指的是索引为1的分数：0.01638886，也就是正例的预测得分。\n",
    "\n",
    "average='macro'：二分类时，该参数可以忽略。用于多分类，\n",
    "' micro '：将标签指标矩阵的每个元素看作一个标签，计算全局的指标。\n",
    "' macro '：计算每个标签的指标，并找到它们的未加权平均值。这并没有考虑标签的不平衡。\n",
    "'weighted '：计算每个标签的指标，并找到它们的平均值，根据支持度(每个标签的真实实例的数量)进行加权。\n",
    "\n",
    "+ sample_weight=None：样本权重。形状(n_samples，)，默认=无。\n",
    "\n",
    "+ max_fpr=None：\n",
    "\n",
    "+ multi_class='raise'：\n",
    "\n",
    "+ labels=None：\n",
    "\n",
    "* 输出：\n",
    "\n",
    "+ auc：是一个float的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model RF: 0.496448278 seconds\n",
      "Accuracy     0.949\n",
      "Precision    0.735\n",
      "Recall       0.735\n",
      "F1_score     0.735\n",
      "AUC          0.925\n",
      "AUPRC        0.566\n",
      "Name: non-avg, dtype: object\n",
      "[[307   9]\n",
      " [  9  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       316\n",
      "           1       0.74      0.74      0.74        34\n",
      "\n",
      "    accuracy                           0.95       350\n",
      "   macro avg       0.85      0.85      0.85       350\n",
      "weighted avg       0.95      0.95      0.95       350\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier     #随机森林\n",
    "def RFC2(trainData,trainLabels, testData,testLabels,paramdict):\n",
    "    \n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)#二值化\n",
    "    #paramdict={'n_estimators':63, 'max_depth':5,'criterion':'entropy','class_weight':'balanced','random_state':42}\n",
    "    rfc = RandomForestClassifier(n_estimators=paramdict['n_estimators'], \n",
    "                                 max_features=paramdict['max_features'],\n",
    "                                 min_samples_split=paramdict['min_samples_split'],\n",
    "                                 min_samples_leaf=paramdict['min_samples_leaf'], \n",
    "                                 min_weight_fraction_leaf=paramdict['min_weight_fraction_leaf'],\n",
    "                                 criterion=paramdict['criterion'],\n",
    "                                 class_weight='balanced',random_state=paramdict['random_state']) \n",
    "    #rfc = RandomForestClassifier(n_estimators=63, max_depth=5,criterion='entropy',class_weight='balanced',random_state=42)\n",
    "    start = time()\n",
    "    rfc = rfc.fit(trainData, trainLabels)                 #用训练集数据训练模型\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train model RF: %.9f seconds\" % elapsed)\n",
    "    pred = rfc.predict(testData)\n",
    "    result = rfc.score(testData, testLabels)\n",
    "    proba = rfc.predict_proba(testData)[:,1]\n",
    "    rocauc = roc_auc_score(testLabels, proba)\n",
    "    recall = recall_score(testLabels, pred)\n",
    "    precision = precision_score(testLabels, pred)\n",
    "    print(resultAnalysis(testData,testLabels,pred,proba,))\n",
    "    \n",
    "    return pred\n",
    "#GS.fit(PCA_projected_trainData, train_labels)\n",
    "#print(GS.best_params_)\n",
    "#print(GS.best_score_)\n",
    "\n",
    "parameters={'n_estimators':63, 'max_depth':20,'max_features':26,'min_samples_split':14, 'min_samples_leaf':3,'min_weight_fraction_leaf':0.00,\n",
    "            'criterion':'entropy','class_weight':'balanced','random_state':42}\n",
    "RFC2(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train XGBoost model: 1.617673635 seconds\n",
      "Accuracy      0.92\n",
      "Precision    0.571\n",
      "Recall       0.706\n",
      "F1_score     0.632\n",
      "AUC          0.917\n",
      "AUPRC        0.432\n",
      "Name: non-avg, dtype: object\n",
      "[[298  18]\n",
      " [ 10  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       316\n",
      "           1       0.57      0.71      0.63        34\n",
      "\n",
      "    accuracy                           0.92       350\n",
      "   macro avg       0.77      0.82      0.79       350\n",
      "weighted avg       0.93      0.92      0.92       350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from time import *\n",
    "\n",
    "#def load_csv(path):\n",
    "#    df = pd.read_csv(path)\n",
    "#    target = df['cls']\n",
    "#    df = df.drop(['cls'], axis=1)\n",
    "#    return xgb.DMatrix(df.values, label=target.values)\n",
    "\n",
    "#dtest = load_csv('/hdd/hdd1/twonormData/t1000.csv')\n",
    "#xgb.DMatrix(PCA_projected_trainData, label=train_labels.astype(int))\n",
    "def evaluate(trainData,trainLabels, testData,testLabels, num_trees=100,max_depth=50,num_jobs=-1):\n",
    "    trainLabels=trainLabels.astype(int)\n",
    "    testLabels=testLabels.astype(int)\n",
    "    dtrain = xgb.DMatrix(trainData, label=trainLabels)\n",
    "    dtest = xgb.DMatrix(testData, label=testLabels)\n",
    "    param = {'num_parallel_tree':num_trees, 'max_depth':max_depth, 'objective':'binary:logistic',\n",
    "        'nthread':num_jobs, 'subsample':0.7,'scale_pos_weight':9}\n",
    "    start = time()\n",
    "    model = xgb.train(param, dtrain, 1)\n",
    "    # 性能评估以XGboost为例\n",
    "    #xgb = xgb.XGBClassifier()\n",
    "    # 对训练集训练模型\n",
    "    #xgb.fit(trainData,trainLabels)\n",
    "    # 对测试集进行预测\n",
    "    #y_pred = xgb.predict(testData)\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train XGBoost model: %.9f seconds\" % elapsed)\n",
    "    prediction = model.predict(dtest)\n",
    "    #proba = model.predict_proba(testData)[:,1]\n",
    "    #print(prediction.shape)\n",
    "    length=len(prediction)\n",
    "    proba = np.stack([[1-a for a in prediction],prediction],axis=-1)\n",
    "    pred=[]\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] >0.5:\n",
    "            pred.append(1)\n",
    "        else:pred.append(0)\n",
    "    \n",
    "    #print(pred)\n",
    "    #print(\"Accuracy = %.3f\" % np.mean(prediction == dtest.get_label()))\n",
    "    print(resultAnalysis(testData,testLabels,pred,prediction))\n",
    "\n",
    "evaluate(PCA_projected_trainData,train_labels, PCA_projected_testData,test_labels,)    #choose your own parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参是一个非常重要的步骤，可以帮助我们优化机器学习算法的表现。对于sklearn随机森林算法，调参需要注意以下几点：\n",
    "\n",
    "n_estimators：随机森林中树的数量。增加这个参数可以提高算法的表现，但同时会使得算法的计算时间变长。因此，需要根据具体情况进行调整。\n",
    "\n",
    "max_depth：树的最大深度。增加这个参数可以提高算法的表现，但同时也会使得算法的计算时间变长。需要注意的是，如果max_depth设置太大，会导致过拟合的问题。\n",
    "\n",
    "min_samples_split：节点进行分裂所需的最小样本数量。增加这个参数可以防止过拟合，但会使得算法的表现变差。\n",
    "\n",
    "min_samples_leaf：叶节点所需的最小样本数量。增加这个参数可以防止过拟合，但会使得算法的表现变差。\n",
    "\n",
    "max_features：特征随机选择的个数。增加这个参数可以提高算法的表现，但也会增加算法的计算时间。\n",
    "\n",
    "bootstrap：是否有放回地进行采样。如果设置为False，会使得随机森林变成极端随机森林。\n",
    "\n",
    "criterion：衡量分裂质量的准则。一般选择“gini”或“entropy”。\n",
    "\n",
    "## Grid Search-网格搜索\n",
    "调参的过程可以使用网格搜索(Grid Search)或随机搜索(Random Search)等方法进行。可以通过如下代码进行调参：\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# 设置要调整的参数\n",
    "parameters = {'n_estimators': [10, 50, 100],\n",
    "              'max_depth': [1, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2],\n",
    "              'max_features': [2, 4],\n",
    "              'bootstrap': [True, False],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# 使用网格搜索进行调参\n",
    "grid_search = GridSearchCV(clf, parameters, cv=3, n_jobs=1, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 输出最佳的参数组合和得分\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "```\n",
    "在网格搜索的过程中，我们首先创建了一个空的随机森林分类器，然后设置了要调整的参数。我们还设置了交叉验证的次数(cv)和并行处理的数量(n_jobs)。然后我们使用GridSearchCV函数进行网格搜索，并使用了verbose参数指定详细程度。最后输出最佳的参数组合和得分。\n",
    "\n",
    "## 贝叶斯优化\n",
    "scikit-learn里的内置贝叶斯优化库为BayesSearchCV，该库可以在超参数空间中使用朴素贝叶斯优化算法，以尝试找到最佳超参数组合。BayesSearchCV需要设定要优化的超参数的搜索范围和目标指标，并为算法提供模型评估器。\n",
    "\n",
    "下面是一个简单的随机森林模型的贝叶斯优化调参示例代码：\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# 定义随机森林模型并设定超参数空间\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "param_dist = {'max_depth': Integer(10, 50),\n",
    "              'min_samples_split': Integer(2, 10),\n",
    "              'min_samples_leaf': Integer(1, 10),\n",
    "              'max_features': Real(0.1, 1.0)}\n",
    "\n",
    "# 使用交叉验证来对模型进行评估\n",
    "def objective_func(params):\n",
    "    rf.set_params(**params)\n",
    "    return -np.mean(cross_val_score(rf, X, y, cv=5, scoring='f1'))\n",
    "\n",
    "# 在超参数空间上运用贝叶斯搜索来优化模型\n",
    "opt = BayesSearchCV(rf,\n",
    "                    param_dist,\n",
    "                    n_iter=25,\n",
    "                    scoring='f1',\n",
    "                    cv=5,\n",
    "                    random_state=0)\n",
    "opt.fit(X, y)\n",
    "```\n",
    "在上面的代码中，使用了sklearn的内置贝叶斯优化库BayesSearchCV。首先定义了一个随机森林模型并设定了超参数空间，然后使用交叉验证评估模型的表现，并将评估结果作为目标函数。最后，在超参数空间上使用BayesSearchCV进行参数搜索，搜索结束后可以使用如下代码来输出参数搜索的结果：\n",
    "\n",
    "```python\n",
    "print(\"Best F1 Score:3f\" opt.best_score_)\n",
    "print(\"Best Parameters:s\" str(opt.best_params_))\n",
    "```\n",
    "使用BayesSearchCV时，可以通过参数n_iter来控制贝叶斯优化的迭代次数，通常n_iter要比随机搜索大，这样可以更好地探索超参数空间。同时，参数cv指定了用于交叉验证的数据集的拆分数量，一般默认为5。scoring参数用于指定评估指标，这里选择了F1得分，因为它同时考虑了准确率和召回率，是一个比较全面的指标。\n",
    "\n",
    "需要注意的是，由于贝叶斯优化是一种基于概率的优化方法，因此它的寻优速度可能会受到几个因素的影响，例如超参数的数量和范围、目标函数的复杂性等。因此，在进行调参时，需要根据实际情况和计算资源情况进行适当的调整和控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score:0.303571\n",
      "Best Parameters:OrderedDict([('criterion', 'gini'), ('max_depth', 18), ('min_samples_leaf', 1), ('min_samples_split', 2), ('min_weight_fraction_leaf', 0.0), ('n_estimators', 227)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from functools import partial\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "X=PCA_projected_trainData\n",
    "y=train_labels.astype(int)\n",
    "\n",
    "# 定义随机森林模型并设定超参数空间\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {'max_depth': Integer(1, 20),\n",
    "              'n_estimators': Integer(100, 250),\n",
    "              'min_samples_split': Integer(2, 10),\n",
    "              'min_samples_leaf': Integer(1, 10),\n",
    "              'min_weight_fraction_leaf': Real(0, 0.5),\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              }\n",
    "\n",
    "# 使用交叉验证来对模型进行评估\n",
    "def objective_func(params):\n",
    "    rf.set_params(**params)\n",
    "    return -np.mean(cross_val_score(rf, X, y, cv=10, scoring='f1'))\n",
    "\n",
    "# 在超参数空间上运用贝叶斯搜索来优化模型\n",
    "opt = BayesSearchCV(rf,\n",
    "                    param_dist,\n",
    "                    n_iter=35,\n",
    "                    scoring='recall',\n",
    "                    cv=10,\n",
    "                    random_state=42)\n",
    "opt.fit(X, y)\n",
    "print(\"Best F1 Score:%3f\" % opt.best_score_)\n",
    "print(\"Best Parameters:%s\" % str(opt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_split': 9}\n",
      "0.9163436078484622\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':np.arange(1,20,1),'min_samples_split':np.arange(2, 2+20, 1)} \n",
    "#param_grid ={'max_features':np.arange(5,30,1),'min_samples_leaf':np.arange(1, 1+10, 1),'min_samples_split':np.arange(2, 2+20, 1),'criterion':['gini', 'entropy']}\n",
    "rfc = RandomForestClassifier(n_estimators=120,oob_score=True,class_weight='balanced',criterion='gini',random_state=42) \n",
    "GS = GridSearchCV(rfc,param_grid,cv=10,scoring='roc_auc') \n",
    "GS.fit(PCA_Data,all_labels.astype(int))\n",
    "print(GS.best_params_)\n",
    "print(GS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting scikit-optimize\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/55/f6/2d9efbd86126c40fe0f8a47611a9e2480b493b6f0ce9751bdf0240cfa091/scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
      "     ------------------------------------ 100.3/100.3 kB 240.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\software\\anaconda\\lib\\site-packages (from scikit-optimize) (1.21.5)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/35/fd/78a3a11c7b9b11878ebbf4461a09cbc758bdfc1b45168972727f7334b09a/pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\software\\anaconda\\lib\\site-packages (from scikit-optimize) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\software\\anaconda\\lib\\site-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\software\\anaconda\\lib\\site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: PyYAML in d:\\software\\anaconda\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\software\\anaconda\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.2.0)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  66  83 100 116 133 150 166 183 200]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "[i for i in range(50,200,10)]\n",
    "step = 10\n",
    "variable = Integer(low=50, high=200, prior='uniform')\n",
    "\n",
    "# Discretize the variable into step bins\n",
    "variable = np.linspace(variable.low,variable.high,step).astype(int)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pip in d:\\software\\anaconda\\lib\\site-packages (23.0.1)\n",
      "Collecting install\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4d/c8/8cbca135f9e167810756ea2bc34b028501936675fcbd7dadccf752fa4622/install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Collecting dolphindb\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/0a/e04ff775b240e77e29c7e830b0a4ffac06efcf661996924f7328add29429/dolphindb-1.30.21.1-cp37-cp37m-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas!=1.3.0,<=1.5.2,>=0.25.1 in d:\\software\\anaconda\\lib\\site-packages (from dolphindb) (1.3.5)\n",
      "Requirement already satisfied: numpy<=1.23.4,>=1.18 in d:\\software\\anaconda\\lib\\site-packages (from dolphindb) (1.21.5)\n",
      "Requirement already satisfied: future in d:\\software\\anaconda\\lib\\site-packages (from dolphindb) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\software\\anaconda\\lib\\site-packages (from pandas!=1.3.0,<=1.5.2,>=0.25.1->dolphindb) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\software\\anaconda\\lib\\site-packages (from pandas!=1.3.0,<=1.5.2,>=0.25.1->dolphindb) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\software\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,<=1.5.2,>=0.25.1->dolphindb) (1.16.0)\n",
      "Installing collected packages: install, dolphindb\n",
      "Successfully installed dolphindb-1.30.21.1 install-1.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pip install dolphindb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
